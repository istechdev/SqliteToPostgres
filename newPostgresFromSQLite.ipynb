{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook para la conversi贸n de db sqlite to pg**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 0: Inicializacion - Definici贸n de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importacion de librerias*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy as al\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Configuraciones*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para la eExtraccion de parametros del archivo 'sqliteToPG-conf.ini' en secciones\n",
    "def config(archivo='sqliteToPG-conf.ini', seccion='pg_server'):\n",
    "    # Crear el parser y leer el archivo\n",
    "    parser = ConfigParser()\n",
    "    parser.read(archivo)\n",
    " \n",
    "    # Obtener la secci贸n de conexi贸n a la base de datos\n",
    "    result = {}\n",
    "    if parser.has_section(seccion):\n",
    "        params = parser.items(seccion)\n",
    "        for param in params:\n",
    "            result[param[0]] = param[1]\n",
    "        return result\n",
    "    else:\n",
    "        raise Exception('Seccion {0} no encontrada en el archivo {1}'.format(seccion, archivo))\n",
    "\n",
    "params_sqlite = config('sqliteToPG-conf.ini','sqlite') # Se extraen los parametros de la base SQlite de sqliteToPG-conf.ini\n",
    "params_pg_server = config() # Se extraen los parametros del servidor postgres de sqliteToPG-conf.ini\n",
    "params_newDB = config('sqliteToPG-conf.ini','pg_new_db') # Se extraen los parametros de la nueva base postgres de sqliteToPG-conf.ini\n",
    "# No usar mayusculas para el nombre de la base de datos postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 1: Sqlite Dump Scheme and data to Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece la conexion con una BDD SQlite\n",
    "conn = sqlite3.connect(params_sqlite['sqlite_db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de dataframe 'dbSchemaQueries' con sentencias SQL para la creacion del esquema de la BDD\n",
    "dbTableQueries = pd.read_sql(\"select sql from sqlite_master\", con=conn)\n",
    "dbTableQueries = dbTableQueries.mask(dbTableQueries.eq(None)).dropna()\n",
    "\n",
    "# Definicion de dataframe 'tableNames' con los nombres de las tablas\n",
    "tableNames = pd.read_sql(\"SELECT name FROM sqlite_master WHERE (type='table')\", con=conn)\n",
    "\n",
    "# Definicion de 'dbData', lista que contendra multiples dataframes, cada uno con los datos de una tabla de la BDD\n",
    "dbData = []\n",
    "selectQuery = \"SELECT * from \"\n",
    "\n",
    "# Iteracion en la que se carga 'dbData' con los datos de cada tabla de la BDD (un SELECT por tabla existente)\n",
    "for i in range(len(tableNames)):\n",
    "    currentTable = tableNames['name'][i]\n",
    "    dbData.append(pd.read_sql_query(selectQuery + currentTable, con=conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cierra la conexion con una BDD SQlite\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 2: Datatypes convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define una lista que almacena los valores de start para las PK de la nueva BDD\n",
    "startValues = []\n",
    "\n",
    "# Recorre dataframe a dataframe\n",
    "for i in dbData:\n",
    "\n",
    "    # Recorre las columnas del dataframe actual\n",
    "    for columnName in i.columns: \n",
    "\n",
    "        # Cambia los datos de la columna a boolean\n",
    "        if \"(BL)\" in columnName:\n",
    "            i[columnName] = i[columnName].astype(bool)\n",
    "            i.rename(columns = {columnName:columnName.replace('(BL)','')}, inplace = True)\n",
    "\n",
    "        # Cambia los datos de la columna a datetime para su posterior almacenamiento\n",
    "        \"\"\"elif \"date\" in columnName:\n",
    "            i[columnName] = pd.to_datetime(i[columnName])\n",
    "            #print(i[columnName])\"\"\"\n",
    "\n",
    "        # Busca el ultimo valor del campo id almacenado y guarda el valor siguiente, en caso de no existir id, almacena un 1\n",
    "        if columnName == 'id':\n",
    "            if i.empty:\n",
    "                startValues.append(1)  \n",
    "            else: \n",
    "                startValues.append(i.at[i.index[-1], columnName]+1)\n",
    "\n",
    "print(startValues)\n",
    "\n",
    "\"\"\"for i in dbData:\n",
    "    for columnName in i.columns: \n",
    "        print (columnName)\"\"\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 3: Scheme Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de funcion 'boolCHECKDelete' (limpia una Query SQL eliminando los CHECKS booleanos que pueda tener)\n",
    "def boolCHECKDelete(query):\n",
    "\n",
    "    # Define una variable auxiliar para procesar la query recibida\n",
    "    parsedQuery = query\n",
    "\n",
    "    # Define un substring con todos los CHECK de la query\n",
    "    allChecks = parsedQuery[parsedQuery.index('\\tCHECK'):-1]\n",
    "\n",
    "    # Separa cada CHECK como un nuevo substring\n",
    "    allChecks = allChecks.split('\\n')\n",
    "\n",
    "    # Recorre todos los CHECK que posee la query, busca si existe el tag '(BL)' y borra los que los posean (boolean CHECKS utilizados en SQlite) \n",
    "    for i in range(len(allChecks)):\n",
    "        if '(BL)' in allChecks[i]:\n",
    "            parsedQuery = parsedQuery.replace('\\n'+allChecks[i], '')\n",
    "\n",
    "    # Chequea si existe una ',' en el final de la query y la elimina para mantener correcta la sintaxis de la misma\n",
    "    if ',' in parsedQuery[len(parsedQuery)-5:]:\n",
    "        parsedQuery = parsedQuery[:len(parsedQuery)-5] + '' + parsedQuery[len(parsedQuery)-5] + '\\n)'\n",
    "    return parsedQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dbTableQueries['sql'][4]\n",
    "#a = a.replace('CREATE UNIQUE INDEX ', 'CREATE UNIQUE INDEX '+ params_newDB['new_schema_name']+'.')\n",
    "#a = a.replace('ON ', 'ON '+ params_newDB['new_schema_name']+'.')\n",
    "a = a.replace('CREATE TABLE ', 'CREATE TABLE '+ params_newDB['new_schema_name']+'.')\n",
    "a = a.replace('REFERENCES ', 'REFERENCES '+ params_newDB['new_schema_name']+'.')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parseo de todas las queries de la BDD SQlite almacenadas en el dataframe 'dbSchemaQueries'\n",
    "\n",
    "# Definicion de 'parsed_dbSchemaQueries', lista que contendra todas las queries parseadas\n",
    "parsed_dbTableQueries = []\n",
    "startValuesCount = 0\n",
    "\n",
    "# Iteracion que recorre todo el dataframe, query a query.\n",
    "for i in dbTableQueries.index:\n",
    "\n",
    "    # Define una variable auxiliar para procesar la query actual\n",
    "    currentQuery = dbTableQueries['sql'][i]\n",
    "    \n",
    "    # Reemplaza los campos DATETIME con TIMESTAMP para el pasaje a PosgreSQL\n",
    "    currentQuery = currentQuery.replace('DATETIME', 'TIMESTAMP')\n",
    "\n",
    "    # Sentencia ocasional debido al fallo en un campo de la tabla purchase, donde la longitud de los datos almacenados supera la asignada al campo\n",
    "    currentQuery = currentQuery.replace('VARCHAR(16)', 'VARCHAR(32)')\n",
    "\n",
    "    # Agrega el nombre de el Schema al que perteneceran las tablas\n",
    "    currentQuery = currentQuery.replace('CREATE TABLE ', 'CREATE TABLE '+ params_newDB['new_schema_name']+'.')\n",
    "    currentQuery = currentQuery.replace('REFERENCES ', 'REFERENCES '+ params_newDB['new_schema_name']+'.')\n",
    "\n",
    "    if('CREATE UNIQUE INDEX' in currentQuery or 'CREATE INDEX' in currentQuery):\n",
    "        #currentQuery = currentQuery.replace('CREATE UNIQUE INDEX ', 'CREATE UNIQUE INDEX '+ params_newDB['new_schema_name']+'.')\n",
    "        currentQuery = currentQuery.replace('ON ', 'ON '+ params_newDB['new_schema_name']+'.')\n",
    "\n",
    "    # Reemplaza los campos id para ser autoincrementales\n",
    "    if 'id INTEGER NOT NULL' in currentQuery:\n",
    "        # Setea el tipo de Identity, el valor de Start e incremento que tendra la tabla\n",
    "        currentQuery = currentQuery.replace('id INTEGER NOT NULL,', 'id INTEGER GENERATED BY DEFAULT AS IDENTITY\\n\\t(START WITH ' + str(startValues[startValuesCount]) + ' INCREMENT BY 1),')\n",
    "        startValuesCount = startValuesCount + 1\n",
    "\n",
    "\n",
    "    # Conditions for table named \"user\".\n",
    "    \"\"\"currentQuery = currentQuery.replace('CREATE TABLE user', 'CREATE TABLE \"user\"')\n",
    "    currentQuery = currentQuery.replace('CREATE TABLE \"user\"_', 'CREATE TABLE user_')\n",
    "    currentQuery = currentQuery.replace('ON user', 'ON \"user\"')\n",
    "    currentQuery = currentQuery.replace('ON \"user\"_', 'ON user_')\n",
    "    currentQuery = currentQuery.replace('REFERENCES user', 'REFERENCES \"user\"')\n",
    "    currentQuery = currentQuery.replace('REFERENCES \"user\"_', 'REFERENCES user_')\"\"\"\n",
    "\n",
    "\n",
    "    # Se verifica que la query posea CHECKS, en caso de tenerlos, ejecuta 'boolCHECKDelete' en la query actual\n",
    "    if(currentQuery.find('CHECK') != -1):\n",
    "        currentQuery = boolCHECKDelete(currentQuery)\n",
    "\n",
    "    # A帽ade la query parseada a la lista\n",
    "    currentQuery = currentQuery.replace('(BL)', '')\n",
    "    parsed_dbTableQueries.append(currentQuery)\n",
    "\n",
    "# Impresion por pantalla de las queries parseadas\n",
    "for i in range(len(parsed_dbTableQueries)):\n",
    "    print(parsed_dbTableQueries[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 4: Conexi贸n al servidor de bases de datos PostgreSQL y Creaci贸n de nueva BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cursor con el objeto conexion\n",
    "conexion = psycopg2.connect(**params_pg_server)\n",
    "conexion.autocommit = True\n",
    "cur = conexion.cursor()\n",
    "\n",
    "# creamos la nueva BDD utilizando el cursor\n",
    "createDBQuery = 'CREATE DATABASE ' + params_newDB['new_pg_db_name']\n",
    "cur.execute(createDBQuery)\n",
    "\n",
    "# crea nuevos parametros para conectarse a la BDD recien creada\n",
    "#newDBparams['database'] = newPostgreSQLDbName\n",
    "#newDBparams['user'] = 'gdeluca'\n",
    "#newDBparams['password'] = 'gdeluca'\n",
    "\n",
    "# Cerramos la conexi贸n\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 5: Creacion del nuevo Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cursor con el objeto conexion\n",
    "aux_params_pg_server = params_pg_server.copy()\n",
    "aux_params_pg_server['database'] = params_newDB['new_pg_db_name']\n",
    "conexion = psycopg2.connect(**aux_params_pg_server)\n",
    "conexion.autocommit = True\n",
    "cur = conexion.cursor()\n",
    "\n",
    "# Creamos la query para generar el nuevo Schema\n",
    "createSchemaQuery = 'CREATE SCHEMA ' + params_newDB['new_schema_name'] + ';'\n",
    "\n",
    "#Ejecutamos la query en la bdd creada\n",
    "cur.execute(createSchemaQuery)\n",
    "\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 6: Creacion de las tablas en la nuevo Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cursor con el objeto conexion\n",
    "#aux_params_pg_server = params_pg_server.copy()\n",
    "#aux_params_pg_server['database'] = params_newDB['new_pg_db_name']\n",
    "conexion = psycopg2.connect(**aux_params_pg_server)\n",
    "conexion.autocommit = True\n",
    "cur = conexion.cursor()\n",
    "\n",
    "# iteracion en la cual el cursor ejecuta todas las queries previamente parseadas una a una. (Crea las tablas en la nueva BDD PostgreSQL)\n",
    "for i in range(len(parsed_dbTableQueries)):\n",
    "    cur.execute(parsed_dbTableQueries[i])\n",
    "\n",
    "# Cerramos la conexi贸n\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 7: Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una conexion en 'engine' para utilizar SQLAlchemy\n",
    "dbConexionString = 'postgresql://'+params_pg_server['user']+':'+params_pg_server['password']+'@'+params_pg_server['host']+':'+params_pg_server['port']+'/' + params_newDB['new_pg_db_name']\n",
    "engine = al.create_engine(dbConexionString)\n",
    "\n",
    "#for i in tableNames.index:\n",
    "#    tableNames['name'][i] = tableNames['name'][i].replace(tableNames['name'][i], params_newDB['new_schema_name'] + '.' + tableNames['name'][i])\n",
    "\n",
    "with engine.begin() as connection:\n",
    "    # Iteracion donde recorre cada dataframe en 'dbData' y carga los datos formateados de cada df a la tabla correspondiente en la nueva BDD\n",
    "    for i in range(len(dbData)):\n",
    "        dbData[i].to_sql(tableNames['name'][i], con=connection, if_exists=\"append\", index = False, schema=params_newDB['new_schema_name'],)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerramos el engine de SQLAlchemy\n",
    "connection.close()\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f1b688e5dc678ecae4edbf458b512dd7d49e79593819bf17fc264de6db80c5b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
